{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 3: Introduction to machine learning algorithms\n",
    "\n",
    "In this exercise worksheet, you'll implement machine learning algorithm models from scratch to extract biological meaning from sequence data. We will be focusing on the 2 following algorithms:\n",
    "\n",
    "1. Decision tree to predict whether a breast cancer tumor is malignant or not from its visual properties.\n",
    "2. Neural network to predict whether a DNA sequence is from a promoter or not.\n",
    "\n",
    "We'll also focus on the strength and weaknesses of these models, how to assess their results and potential ways to improve them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to numpy: work with arrays and matrices in python\n",
    "\n",
    "When doing machine learning, we often have matrices containing multiple samples and features. The python package numpy is very helpful to manipulate this type of data. It also has extensive documentation, which is very helpful if you get stuck : https://numpy.org/doc/1.19/\n",
    "\n",
    "Below is a quick demonstration of its use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpy allows to easily manipulate arrays in one or more dimensions\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "my_list = [\n",
    "    [1,   2,   3  ],\n",
    "    [10,  20,  30 ],\n",
    "    [100, 200, 300],\n",
    "] # Standard python 2D list\n",
    "my_array = np.array(my_list) # Numpy aray equivalent\n",
    "\n",
    "# How to select the first column of the 2D array ?\n",
    "\n",
    "# Base python version\n",
    "first_column = [0, 0, 0]\n",
    "for i, row in enumerate(my_list):\n",
    "    first_column[i] = row[0]\n",
    "\n",
    "# Numpy version\n",
    "first_column = my_array[:, 0] # We can slice the array in two dimensions: [rows, cols]\n",
    "\n",
    "# How to multiply every element in the array by 10 ?\n",
    "\n",
    "# Base python version\n",
    "for i in range(len(my_list)):\n",
    "    for j in range(len(my_list[0])):\n",
    "        my_list[i][j] *= 10\n",
    "        \n",
    "# Numpy version\n",
    "my_array *= 10\n",
    "\n",
    "# Note: What would happen if you tried running my_list * 10 ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_array.shape:  (3, 3)\n",
      "my_array.size:  9\n"
     ]
    }
   ],
   "source": [
    "# You can display various informations on numpy arrays\n",
    "print(\"my_array.shape: \", my_array.shape)\n",
    "print(\"my_array.size: \", my_array.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example above means operations are vectorized on numpy arrays. This means we do not need to write the loops explicitely. They are still executed implicitely, but in C code, which is much faster ! See for yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.6 ms ± 777 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n",
      "1.56 ms ± 76.9 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "big_list = [[3] * 1000] * 1000\n",
    "big_array = np.array(big_list)\n",
    "%timeit [[v * 10 for v in row] for row in big_list]\n",
    "%timeit big_array * 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Decision trees\n",
    "\n",
    "As we saw in the lectures, decision trees are easily interpretable. Here we will implement a decision tree model for classification, from scratch. You will need to implement all the necessary building blocks:\n",
    "\n",
    "* A cost function allowing to compute the purity of nodes and rate splits using information gain.\n",
    "* An algorithm to find the feature and value providing best binary split.\n",
    "* A recursive function that will call itself to split each node into children.\n",
    "\n",
    "Some of the code is already written to guide you, but you still need to implement the critical part.\n",
    "\n",
    "**a) Implement a function to compute the entropy of an array. We will later use it to rate splits.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def entropy(array, possible_values):\n",
    "    \"\"\"\n",
    "    Compute the entropy of an array of values\n",
    "    Hint: To get a maximum value of 1 like in the\n",
    "    example, you need to use a log of base 2 (np.log2)\n",
    "    >>> entropy([\"A\", \"A\", \"A\"], [\"A\", \"B\"])\n",
    "    0\n",
    "    >>> entropy([1, 2, 2, 1], [1, 2])\n",
    "    1\n",
    "    \"\"\"\n",
    "    if not len(array): return 0\n",
    "    probs = [0] * len(possible_values)\n",
    "    entropy = 0\n",
    "    \n",
    "    ...\n",
    "    \n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "### TEST YOUR CODE BY RUNNING THIS CELL ###\n",
    "###########################################\n",
    "assert entropy([\"A\", \"A\", \"A\"], [\"A\", \"B\"]) == 0\n",
    "assert entropy([1, 2, 2, 1], [1, 2]) > entropy([1, 2, 1, 1], [1, 2])\n",
    "assert entropy([1, 2, 2, 1], [1, 2]) > 0\n",
    "print(' 0 0 0 \\n0 . . 0\\n0  v  0\\n 0 0 0 ')\n",
    "print(\"Congrats !!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**b) Read the specification of our tree structure in the cell below**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can represent out tree with nested dictionaries. Each node is a dictionary with two children {{...},{...}}.\n",
    "### Example of a tree with 5 nodes (2 internal and 3 terminal):\n",
    "\"\"\"\n",
    "GRAPHIC REPRESENTATION:\n",
    "\n",
    "      o1      <- root o1, split on feature 3 at value 0.5\n",
    "    /  \\\n",
    "   x1    o2   <- internal node o2, split on feature 1 at value 03\n",
    "       / \\\n",
    "      x2   x3 <- terminal nodes x1, x2 and x3 contain the prediction result\n",
    "\"\"\"\n",
    "\n",
    "# DICTIONARY REPRESENTATION:\n",
    "\n",
    "# Each internal node has attributes describing what feature was used for the split\n",
    "# and what was the optimal value. Internal nodes also have 'left' and 'right' attributes,\n",
    "# which each contain another dictionary representing the children nodes.\n",
    "\n",
    "# Terminal nodes instead have data and pred attributes, which indicate which training samples\n",
    "# are in the node, and what is the prediction.\n",
    "\n",
    "dummy_tree  = {\n",
    "    'split_feature': 3,         #\n",
    "    'split_value': 0.5,         # ROOT o1\n",
    "    'depth': 1,                 #\n",
    "    'left':                     \n",
    "        {\n",
    "            'depth': 2,           # TERMINAL x1\n",
    "            'data':[1, 2, 4],     #\n",
    "            'pred': \"A\",          #\n",
    "        },\n",
    "    'right':\n",
    "        {\n",
    "            'split_feature': 1,   # INTERNAL o2\n",
    "            'split_value': -3,    #\n",
    "            'depth': 2,           #\n",
    "            'left':\n",
    "                {\n",
    "                    'depth': 3,     #\n",
    "                    'data': [0],    #  TERMINAL x2\n",
    "                    'pred': \"A\",    #\n",
    "                },\n",
    "            'right': \n",
    "                {\n",
    "                    'depth': 3,     #\n",
    "                    'data': [3, 5], # TERMINAL x3\n",
    "                    'pred': \"B\",    #\n",
    "                },\n",
    "        },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) Write a function to find the best split on the dataset provided below. It contains measurements from breast cancer tumors, and whether they are malignant (1) of begnin (0). The feature matrix Nxp is stored in `X`. The labels are stored in `y`.**\n",
    "> Note: Given a dataset X of N samples (rows) and p features (cols), and target values (labels) y, the algorithm should find the combination of feature j and value s which provides the best split. The best split is defined as the one maximizing the information gain IG.\n",
    "$$IG= entropy(y) - \\frac{1}{(N_l+N_r)}\\left(N_l * entropy(y_l) + N_l * entropy(y_r)\\right)$$\n",
    "where N represents the number of samples in a node, and {l,r} are the left and right children nodes generated by the split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a dummy dataset\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "dataset = datasets.load_breast_cancer()\n",
    "X, y = dataset.data, dataset.target\n",
    "print(\n",
    "    f\"We have N={X.shape[0]} samples, each with p={X.shape[1]} features. The target values are {dataset['target_names']}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split(X, y):\n",
    "    \"\"\"\n",
    "    Select the best split point on dataset X to separate target y.\n",
    "    Returns a dictionary (node) with attributes 'split_features' and\n",
    "    'split_value', describing the split chosen, as well as 'left' and\n",
    "    'right', storing the content of the children nodes. 'left' and 'right'\n",
    "    each contain a list of two elements: the feature matrix and the target array,\n",
    "    i.e. 'left': [Xl, yl], 'right': [Xr, yr].\n",
    "    \n",
    "    >>> get_split(np.array([[1, 1, 2],[1,1,4]]),np.array([1,2]))\n",
    "    {\n",
    "        'split_feature': 2,\n",
    "        'split_value': 2,\n",
    "        'left':  [np.array([[1, 1, 2]]), np.array([1])],\n",
    "        'right': [np.array([[1, 1, 4]]), np.array([2])],\n",
    "    }\n",
    "    \"\"\"\n",
    "    y_values = set(y)\n",
    "    old_entropy = entropy(y, y_values)\n",
    "    best_value = best_feature = None\n",
    "    best_score =  -np.inf\n",
    "    new_node = {}\n",
    "    # X.shape[1] gives the number of columns (features) in X\n",
    "    for feature in range(X.shape[1]):\n",
    "        # We iterate over the possible values for the current feature\n",
    "        for value in set(X[:, feature]):\n",
    "            ...\n",
    "    \n",
    "    return new_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################\n",
    "### TEST YOUR CODE BY RUNNING THIS CELL ###\n",
    "###########################################\n",
    "your_split = get_split(np.array([[1, 1, 2],[1,1,4]]),np.array([1,2]))\n",
    "req_vals = {\n",
    "    'split_feature': 2,\n",
    "    'split_value': 2,\n",
    "    'left':  [np.array([[1, 1, 2]]), np.array([1])],\n",
    "    'right': [np.array([[1, 1, 4]]), np.array([2])],\n",
    "}\n",
    "assert np.all([your_split[k] == req_vals[k] for k in ['split_value', 'split_feature']]) # Check split\n",
    "assert np.all(your_split[k][0] == req_vals[k][0] for k in ['left', 'right']) # Check X subsets\n",
    "assert np.all(your_split[k][1] == req_vals[k][1] for k in ['left', 'right']) # Check y subsets\n",
    "print(' 0 0 0 \\n0 . . 0\\n0  v  0\\n 0 0 0 ')\n",
    "print(\"Congrats !!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c) Function `recurse_split` performs recursive binary partitioning. It is called once by `build_tree`, and then calls itself until it reaches a base condition (pure node, node too small or maximum depth). However, the `recurse_split` function is incomplete ! You need complete the code.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_freq(array):\n",
    "    \"\"\"Returns the most frequent value in a numpy array\"\"\"\n",
    "    values, counts = np.unique(array,return_counts=True)\n",
    "    ind=np.argmax(counts)\n",
    "    return values[ind]\n",
    "\n",
    "\n",
    "def recurse_split(node, depth, min_node_size=4, max_depth=3):\n",
    "    \"\"\"\n",
    "    Given a node that is already split, check for base conditions\n",
    "    min_node_size and max_depth.\n",
    "    \n",
    "    - If the base conditions are reached in the splits, make children\n",
    "      terminal nodes.\n",
    "    \n",
    "    - If the current node is already pure, ignore splits\n",
    "      and make the current node terminal.\n",
    "    \n",
    "    - If none of the base condition was reached, compute optimal split\n",
    "      on the children and recurse further down into the tree.\n",
    "    \"\"\"\n",
    "    Xl, yl = node['left']\n",
    "    Xr, yr = node['right']\n",
    "    del node['left'], node['right']\n",
    "    # Check if all samples went to the same side (one split is empty)\n",
    "    if not Xl.size or not Xr.size:\n",
    "        # The current node is pure, make it a terminal node (leaf)\n",
    "        del node['split_feature'], node['split_value']\n",
    "        node['data'] = [np.concatenate([Xl, Xr]), np.concatenate([yl, yr])]\n",
    "        node['pred'] = most_freq(node['data'][1])\n",
    "        node['depth'] = depth\n",
    "        # We reached a terminal tree -> return to exit recursion\n",
    "        return\n",
    "    \n",
    "    # If our tree has reached max depth, make the children nodes terminal\n",
    "    if depth >= max_depth:\n",
    "        node['left'] = {'depth': depth + 1, 'data': [Xl, yl], 'pred': most_freq(yl)}\n",
    "        node['right'] = {'depth': depth + 1, 'data': [Xr, yr], 'pred': most_freq(yr)}\n",
    "        # We reached a terminal tree -> return to exit recursion\n",
    "        return\n",
    "    \n",
    "    ### COMPLETE THE CODE BELOW ###\n",
    "    \n",
    "    # process left child. If it is too small, make it terminal\n",
    "    if Xl.shape[0] <= min_node_size:\n",
    "        ...\n",
    "    # Otherwise, keep recursing deeper into the tree\n",
    "    else:\n",
    "        ...\n",
    "    # process right child. If it is too small, make it terminal\n",
    "    if Xr.shape[0] <= min_node_size:\n",
    "        ...\n",
    "    # Otherwise, keep recursing deeper into the tree\n",
    "    else:\n",
    "        ...\n",
    " \n",
    "    return\n",
    "\n",
    "def build_tree(X, y, min_node_size=4, max_depth=3):\n",
    "    \"\"\"Given a dataset and associated targets, build a decision tree\"\"\"\n",
    "    # Initialize a tree with the root node and resulting best splits\n",
    "    tree = get_split(X, y)\n",
    "    tree['depth'] = 1\n",
    "    # The function does not return anything, it modifies 'tree'\n",
    "    recurse_split(tree, 1, min_node_size=min_node_size, max_depth=max_depth)\n",
    "    return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d) What feature split provides the most information gain ? What does that imply. ?**\n",
    "> Note: You can use the print_tree function to visualise your tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(node, feature_names, depth=0):\n",
    "    \"\"\"Print a drawing of your tree\"\"\"\n",
    "    if 'split_feature' in node.keys():\n",
    "        j = feature_names[node['split_feature']]\n",
    "        s = node['split_value']\n",
    "        print(f\"{2*depth*' '}[{j} < {s:.3f}]: IG={node['ig']:.3}\")\n",
    "        print_tree(node['left'], feature_names, depth+1)\n",
    "        print_tree(node['right'], feature_names, depth+1)\n",
    "    else:\n",
    "        print('%s[%s]' % ((2*depth*' ', node['pred'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = build_tree(X[:500], y[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_tree(T, dataset.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**e) Write a function to predict new values using your tree. Try to find a measure of success.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_values(tree, X):\n",
    "    \"\"\"\n",
    "    Traverse the tree with new unknown observations\n",
    "    and retrieve the prediction at the leaves\n",
    "    \"\"\"\n",
    "    pred = np.zeros(X.shape[0])\n",
    "    for i in range(X.shape[0]):\n",
    "        node = tree\n",
    "        terminal = False\n",
    "        while not terminal:\n",
    "            ...\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess(T, X, y):\n",
    "    pred = predict_values(T, X)\n",
    "    correct = y == pred\n",
    "    return sum(correct) / len(correct)\n",
    "    \n",
    "print(f\"{100 * assess(T, X[500:], y[500:]):.2f}% of the predictions are correct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**f) How well does the tree generalize to new data ? Can you guess why, and how to solve this ?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = [3, 4, 5, 10, 20]\n",
    "sizes = [10, 20, 50, 100]\n",
    "params = np.zeros((len(depths) * len(sizes), 3))\n",
    "# We can try changing minimum node size and maximum tree depth !\n",
    "i = 0\n",
    "for m in depths:\n",
    "    for n in sizes:\n",
    "        params[i, 0] = m\n",
    "        params[i, 1] = n\n",
    "        params[i, 2] = assess(build_tree(X[:500], y[:500], min_node_size=n, max_depth=m), X[500:], y[500:])\n",
    "        i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mpl.rcParams['legend.fontsize'] = 10\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.scatter(params[:, 0], params[:, 1], params[:, 2], label='prop. correct classif.', c=params[:, 2], cmap='winter')\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"Max depth\")\n",
    "ax.set_ylabel(\"Min node size\")\n",
    "plt.show()\n",
    "best_idx = np.flatnonzero(params[:, 2] == max(params[:, 2]))[0]\n",
    "print(\n",
    "    f\"The best result of {100*params[best_idx, 2]:.2f}% correct\"\n",
    "    f\" classification was obtained with max_depth={params[best_idx,0]}\"\n",
    "    f\" and min_node_size={params[best_idx, 1]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Exercise 3: Predict promoters from DNA sequence with a neural network\n",
    "\n",
    "Promoter regions are regulatory DNA sequences to which specific proteins can bind to trigger the transcription of neighbouring genes. Depending on the exact promoter sequence, proteins will have more or less affinity to it, which allows fine regulation of gene expression.\n",
    "\n",
    "Here, we want to predict whether DNA sequences are promoters or not. You are given a dataset of 106 DNA sequences, each 57bp long. Some of these sequences, labelled \"+\" originate from a known promoter, while the others, labelled \"-\" are from a non promoter region.\n",
    "\n",
    "Here, you need to make the best possible prediction of promoter state from the DNA sequences using a neural network.\n",
    "\n",
    "To implement this neural network, we use matrix operations via numpy !\n",
    "To help you with the theory, you can read this excellent documentation on neural networks:https://ml-cheatsheet.readthedocs.io/en/latest/nn_concepts.html\n",
    "\n",
    "We use the naming conventions as in the drawing, so you can always refer to it:\n",
    "\n",
    "![image.png](images/nn_with_matrices_displayed.png)\n",
    "\n",
    "However, in our network, layer H will have 4 neurons (H1, H2, H3, H4), and layer O a single neuron (O1).\n",
    "\n",
    "The actual neural network code is already written in the cells below. Your goal is understand and modify it however you want to improve results.\n",
    "\n",
    "Here are a few hints (all of those terms are explained in the link above):\n",
    "* We use sigmoid as our activation function, but you could change it.\n",
    "* We use squared error as our cost functions, there are also other options.\n",
    "* The learning rate and number of iterations are important parameters (also look at early stopping)\n",
    "* You can change the number of nodes in the hidden layer by changing the shape of wh.\n",
    "* A few other words which might be helpful: Regularization, drop out, data augmentation\n",
    "\n",
    "At the end of the notebook, there is a cell to assess your model using Leave One Out (LOO) cross validation. This is a robust way to measure the performance of a model, where we train the network on all samples except 1. We then try to predict the single sample that was removed. We repeat the operation for each different sample. In the end, we just look at the proportion of correct predicitions.\n",
    "\n",
    "Good luck !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to pandas: More flexibility with dataframes\n",
    "Pandas is quite similar to numpy,in the sense that it allows to manipulate tabular data and uses vectorized operations. However, it adds more flexibility withdataframes. Dataframes work exactly like in the Rprogramming language, they have columns which have names, and each column can contain different types of data. The pandas package has an excellent documentation here: https://pandas.pydata.org/\n",
    "\n",
    "Here, we use pandas to load the data. Below is a quick explanation of how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>meanAB</th>\n",
       "      <th>custom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>x</td>\n",
       "      <td>5.5</td>\n",
       "      <td>custom_x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>y</td>\n",
       "      <td>11.0</td>\n",
       "      <td>custom_y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>z</td>\n",
       "      <td>16.5</td>\n",
       "      <td>custom_z</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A   B  C  meanAB    custom\n",
       "0  1  10  x     5.5  custom_x\n",
       "1  2  20  y    11.0  custom_y\n",
       "2  3  30  z    16.5  custom_z"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# We can load a dataframe from a text file with pd.read_csv(), or create one directly:\n",
    "df = pd.DataFrame({'A': [1, 2, 3], 'B': [10, 20, 30], 'C': [\"x\", \"y\", \"z\"]})\n",
    "df.head() # head() shows the first few rows of the dataframe\n",
    "# We can select a column by name, or by position\n",
    "df.loc[:, 'A'] # loc selects columns by name\n",
    "df['A'] # This is a shortcut to do the same thing\n",
    "df.iloc[:, 0] # iloc selects columns by index (position)\n",
    "# Instead of taking all rows, we can apply conditions to select a subset\n",
    "df.loc[df['A'] > 1, ['B', 'C']] # Select columns B and C, but only include rows for which A > 1\n",
    "df.iloc[[0, 2], :] # Select rows 0 and 2 of all columns\n",
    "# It is also possible to apply functions on rows or columns\n",
    "df['meanAB'] = df.loc[:, ['A', 'B']].apply(np.mean, axis=1) # Mean of A and B for each row\n",
    "df['custom'] = df['C'].apply(lambda x: 'custom_' + x) # Custom (lambda) function on column C\n",
    "# There are many more powerful features which we will not need here\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>promoter</th>\n",
       "      <th>ID</th>\n",
       "      <th>seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>+</td>\n",
       "      <td>S10</td>\n",
       "      <td>\\t\\ttactagcaatacgcttgcgttcggtggttaagtatgtataat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>+</td>\n",
       "      <td>AMPC</td>\n",
       "      <td>\\t\\ttgctatcctgacagttgtcacgctgattggtgtcgttacaat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>+</td>\n",
       "      <td>AROH</td>\n",
       "      <td>\\t\\tgtactagagaactagtgcattagcttatttttttgttatcat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>+</td>\n",
       "      <td>DEOP2</td>\n",
       "      <td>\\taattgtgatgtgtatcgaagtgtgttgcggagtagatgttagaa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>+</td>\n",
       "      <td>LEU1_TRNA</td>\n",
       "      <td>\\ttcgataattaactattgacgaaaagctgaaaaccactagaatgc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  promoter         ID                                                seq\n",
       "0        +        S10  \\t\\ttactagcaatacgcttgcgttcggtggttaagtatgtataat...\n",
       "1        +       AMPC  \\t\\ttgctatcctgacagttgtcacgctgattggtgtcgttacaat...\n",
       "2        +       AROH  \\t\\tgtactagagaactagtgcattagcttatttttttgttatcat...\n",
       "3        +      DEOP2  \\taattgtgatgtgtatcgaagtgtgttgcggagtagatgttagaa...\n",
       "4        +  LEU1_TRNA  \\ttcgataattaactattgacgaaaagctgaaaaccactagaatgc..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### DATA LOADING ###\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Load the table using pandas\n",
    "dataset = pd.read_csv('data/session_3_promoters.data', header=None, names=['promoter', 'ID', 'seq'])\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>promoter</th>\n",
       "      <th>ID</th>\n",
       "      <th>seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>+</td>\n",
       "      <td>S10</td>\n",
       "      <td>[3, 1, 2, 3, 1, 4, 2, 1, 1, 3, 1, 2, 4, 2, 3, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>+</td>\n",
       "      <td>AMPC</td>\n",
       "      <td>[3, 4, 2, 3, 1, 3, 2, 2, 3, 4, 1, 2, 1, 4, 3, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>+</td>\n",
       "      <td>AROH</td>\n",
       "      <td>[4, 3, 1, 2, 3, 1, 4, 1, 4, 1, 1, 2, 3, 1, 4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>+</td>\n",
       "      <td>DEOP2</td>\n",
       "      <td>[1, 1, 3, 3, 4, 3, 4, 1, 3, 4, 3, 4, 3, 1, 3, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>+</td>\n",
       "      <td>LEU1_TRNA</td>\n",
       "      <td>[3, 2, 4, 1, 3, 1, 1, 3, 3, 1, 1, 2, 3, 1, 3, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  promoter         ID                                                seq\n",
       "0        +        S10  [3, 1, 2, 3, 1, 4, 2, 1, 1, 3, 1, 2, 4, 2, 3, ...\n",
       "1        +       AMPC  [3, 4, 2, 3, 1, 3, 2, 2, 3, 4, 1, 2, 1, 4, 3, ...\n",
       "2        +       AROH  [4, 3, 1, 2, 3, 1, 4, 1, 4, 1, 1, 2, 3, 1, 4, ...\n",
       "3        +      DEOP2  [1, 1, 3, 3, 4, 3, 4, 1, 3, 4, 3, 4, 3, 1, 3, ...\n",
       "4        +  LEU1_TRNA  [3, 2, 4, 1, 3, 1, 1, 3, 3, 1, 1, 2, 3, 1, 3, ..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### DATA PROCESSING / CLEANING ###\n",
    "\n",
    "# Use strip to remove whitespaces (tabs, shown as \\t) from the sequences\n",
    "dataset['seq'] = dataset['seq'].str.strip()\n",
    "\n",
    "# Neural networks can only read numeric inputs, we need to convert DNA into numbers.\n",
    "# Convert letters to numerals actg -> 1234 (Maybe you could find a better encoding ?)\n",
    "dataset['seq'] = dataset['seq'].apply(lambda x: x.translate(str.maketrans(\"actg\", \"1234\")))\n",
    "# Split strings into lists of integers\n",
    "dataset['seq'] = dataset['seq'].apply(lambda x: [int(b) for b in x])\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(dataset.seq.to_list())\n",
    "# Scale our observations for each column (Will make it easier to fit the model)\n",
    "X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "# Transform -/+ into 0/1 labels\n",
    "y = np.array(dataset.promoter == '+', dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1337)\n",
    "# We will use 90% of our dataset to train the network\n",
    "TRAIN_SIZE = int(X.shape[0] * 0.9)\n",
    "train_idx = np.random.choice(range(X.shape[0]), size=TRAIN_SIZE)\n",
    "X_train, y_train = X[train_idx, :], y[train_idx]\n",
    "# Save the other part to assess it afterwards\n",
    "X_test, y_test = np.delete(X, train_idx, axis=0), np.delete(y, train_idx, axis=0)[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    return np.exp(-z) / (1+np.exp(-z))**2\n",
    "\n",
    "\n",
    "def feedforward(X, wh, wo):\n",
    "    \"\"\"\n",
    "    Given input feature matrix X of shape Nxp and weights\n",
    "    for hidden (wh)) and output (wo) layers, send X through\n",
    "    the network to retrieve the folowing values:\n",
    "    - zh: The linear combination of inputs and weights wh\n",
    "    - zo: The linear combination of the hidden layer result (H) and the output layer weights (wo)\n",
    "    - H: The result from the hidden layer, its value is just sigmoid(zh)\n",
    "    - O: The output from our network, its value is sigmoid(zo)\n",
    "    \"\"\"\n",
    "    zh = X @ wh # Z=XW for hidden layer\n",
    "    zo = zh @ wo # Same for output layer\n",
    "    H = sigmoid(zh)\n",
    "    O = sigmoid(zo)\n",
    "    return zh, zo, H, O\n",
    "\n",
    "\n",
    "def backprop(X, targ, wh, wo, lr=0.1):\n",
    "    \"\"\"\n",
    "    The gradient descent process used to train our algorithm.\n",
    "    Given an input feature matrix, predictions from the network, real (target) values\n",
    "    and the network weight which produced the predictions, this function computes\n",
    "    the prediction error, and backpropagates the partial derivative of this error\n",
    "    according to each weight throughout the network. The resulting gradient give\n",
    "    the direction in which each weight should be adjusted. The learning rate (lr)\n",
    "    is a constant determining how much to adjust the weights in that direction.\n",
    "    \"\"\"\n",
    "    # feed forward\n",
    "    zh, zo, H, pred = feedforward(X, wh, wo)\n",
    "    # Compute prediction error\n",
    "    Eo = (pred - targ) * sigmoid_derivative(zo) # Output layer error: Eo = E'(y) * f'(Zo)\n",
    "    Eh = Eo * (wo.T * sigmoid_derivative(zh))   # Hidden layer error: Eh = E0 * w0 * f'(Zh)\n",
    "    # Cost derivative for weights\n",
    "    dWo = Eo.T @ H\n",
    "    dWh = Eh.T @ X\n",
    "\n",
    "    # Update weights\n",
    "    wh -= lr * dWh.T\n",
    "    wo -= lr * dWo.T\n",
    "    return wh, wo\n",
    "\n",
    "\n",
    "def predict(X, wh, wo):\n",
    "    \"\"\"Send a new prediction through the trained network to get the prediction\"\"\"\n",
    "    inp = np.concatenate([np.ones((X.shape[0], 1)), X], axis=1)\n",
    "    H = sigmoid(inp @ wh)\n",
    "    O = sigmoid(H @ wo)\n",
    "    return O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add a first column of 1's (bias), so that: a0+a1*x1+... = a0*1+a1*x1+...\n",
    "inp   = np.concatenate([np.ones((X_train.shape[0], 1)), X_train], axis=1)\n",
    "# Initialize random weights in the network (we have no idea of good values for now)\n",
    "wh    = np.random.rand(inp.shape[1], 4) # Weights of hidden layer\n",
    "wo    = np.random.rand(4, 1) # Weights of output layer, shape[4, 1]\n",
    "tar   = y_train[:, None] # target values\n",
    "# > Note: The shape of wh and wo define the number of hidden nodes\n",
    "\n",
    "zh, zo, H, O = feedforward(inp, wh, wo)\n",
    "# 10 iterations of training\n",
    "for i in range(100000):\n",
    "    # Adjust weights according to prediction errors\n",
    "    wh, wo = backprop(inp, tar, wh, wo, lr=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We predict the test samples using the trained weights\n",
    "O =  predict(X_test, wh, wo)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarise results into 0 / 1 and check which are equal to the real values\n",
    "correct = (O>0.5) == y_test[:, 0]\n",
    "print(\n",
    "    f\"The network has {100*(1-sum(correct) / len(y_test)):.2f}% misclassification rate. \"\n",
    "    f\"({len(y_test) - sum(correct)}/{len(y_test)} wrong)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist([O[O>0.5], O[O<=0.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does increasing the number of training iterations improves results ? How about learning rate and number of nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ASSESS NETWORK WITH CROSS VALIDATION ###\n",
    "# To make sure the network works properly, we need to measure its success with different subset of testing samples\n",
    "# Since both the dataset and network are small, we'll use Leave One Out cross validation:\n",
    "# Take a single sample out from the training set, and try to predict it. We repeat this operation N times\n",
    "# (one for each sample)\n",
    "MAX_ITER = 2000\n",
    "N_HIDDEN = 8\n",
    "scores = np.zeros(MAX_ITER)\n",
    "\n",
    "\n",
    "# New LOO sample for cross validation\n",
    "for n in range(X.shape[0]):\n",
    "    X_train_loo, y_train_loo = np.delete(X, n, axis=0), np.delete(y, n, axis=0)[:, None]\n",
    "    X_train_loo = np.concatenate([np.ones((X_train_loo.shape[0], 1)), X_train_loo], axis=1)\n",
    "    X_test_loo, y_test_loo = X[None, n, :], y[n, None]\n",
    "    # Reset weights\n",
    "    wh     = np.random.rand(X_train_loo.shape[1], N_HIDDEN) # Weights of hidden layer\n",
    "    wo     = np.random.rand(N_HIDDEN,1) # Weights of output layer, shape[4, 1]\n",
    "    zh, zo, H, O = feedforward(X_train_loo, wh, wo)\n",
    "    # Train for current LOO round\n",
    "    for i in range(MAX_ITER):\n",
    "        # Adjust weights according to prediction errors\n",
    "        wh, wo = backprop(X_train_loo, y_train_loo, wh, wo)\n",
    "        # Binarize predictions probs into 1/0\n",
    "        pred = predict(X_test_loo, wh, wo)[:, 0] > 0.5\n",
    "        # Single sample can be either correct or wrong. Store result\n",
    "        scores[i] += pred == y_test_loo\n",
    "    if not n % (X.shape[0]//10):\n",
    "        print(f\"{100*(n/X.shape[0]):.2f}% rounds completed\")\n",
    "\n",
    "# Convert number of correct guesses into proportion of correct LOO guess\n",
    "scores /= X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plt.plot(range(MAX_ITER), scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Your model has a misclassification rate of {100*(1-scores[-1]):2f}% on LOO cross \"\n",
    "    f\"validation after {MAX_ITER} training iterations\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
